An ensemble is a model that combines multiple other models to make better predictions then any single model alone.
 Bagging (Bootstrap Aggregating): Train many models, each on a diffferent random sample of the data
Partial dependence  helps you see how a single feature influences your models predictions, by plotting the effect of that feature while holding everything else constant.
Gradient boosting is a method where models are built one after another, each learning from the mistakes of the previous. Itâ€™s one of the most powerful techniques for structured/tabular data.
Binary Split = Splits data into two groups. yes or no
A deceiosn tree is a model that makes predictions by asking a seriis of yes? and no question about your data
Random forest: Random forest is a collection of many decision tress trained on a random slices of data.



For tabukar data we can use random forest..

Random forests: Data split into branches , and doing like if else...

We use this for to minimize overfitting..




